{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b38d4f64-0cf7-4c90-bddc-cb80891e74c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Synthesize Questions from AI generated Table Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c05d74-f1a9-4499-bbe3-834c0b2fdd5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install mlflow mlflow[databricks] databricks-agents\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f12583fa-6d0d-4162-aa31-d5c8d92500f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "@frpm_ai_comment @satscores_ai_comment @schools_ai_comment \n",
    "based on these three tables, use every column except the 'comment' column, since it is incomplete and outdated. Put more weight on the 'updated_comment' column in addition to other columns, I want to synthesize business intelligence questions similar to these 3 questions.\n",
    "{'What is the highest eligible free rate for K-12 students in the schools in Alameda County?',\n",
    "'Please list the lowest three eligible free rates for students aged 5-17 in continuation schools.',\n",
    "'Please list the zip code of all the charter schools in Fresno County Office of Education.'}\n",
    "\n",
    "Please return 10 such synthetic examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0bd8c43-8c2d-4698-ba23-baacf50b77aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1 – Load & Display Column Metadata From All Three Tables\n",
    "\n",
    "In this step we:\n",
    "1. Read the three metadata tables into Spark DataFrames.\n",
    "2. Drop the obsolete `comment` column.\n",
    "3. Union the three DataFrames together.\n",
    "4. Display the full result so that the LLM can later consume every row from all tables.\n",
    "\n",
    "Because each table only contains one row per column in the underlying datasets, the overall size is small, so scanning the full tables is safe even though we normally limit scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a174a71d-efb1-4e57-8eb0-f36f404273bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Fully qualified table names\n",
    "frpm_tbl = \"yyang.hackathon_2025q3_project_geniepromptautocompletion_genieeng_feindustryhls.frpm_ai_comment\"\n",
    "sat_tbl = \"yyang.hackathon_2025q3_project_geniepromptautocompletion_genieeng_feindustryhls.satscores_ai_comment\"\n",
    "school_tbl = \"yyang.hackathon_2025q3_project_geniepromptautocompletion_genieeng_feindustryhls.schools_ai_comment\"\n",
    "\n",
    "# Load DataFrames and drop obsolete column\n",
    "frpm_df = spark.table(frpm_tbl).drop(\"comment\")\n",
    "sat_df = spark.table(sat_tbl).drop(\"comment\")\n",
    "school_df = spark.table(school_tbl).drop(\"comment\")\n",
    "\n",
    "# Add a source column for traceability\n",
    "frpm_df = frpm_df.withColumn(\"source_table\", F.lit(\"frpm_ai_comment\"))\n",
    "sat_df = sat_df.withColumn(\"source_table\", F.lit(\"satscores_ai_comment\"))\n",
    "school_df = school_df.withColumn(\"source_table\", F.lit(\"schools_ai_comment\"))\n",
    "\n",
    "# Union all\n",
    "combined_df = frpm_df.unionByName(sat_df).unionByName(school_df)\n",
    "\n",
    "# Display full metadata\n",
    "display(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "047a2c26-e371-4d0e-863f-6a9636b6fa0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2 – Synthesize \\# of BI Questions with an LLM\n",
    "\n",
    "We will now:\n",
    "1. Extract the column metadata we displayed in Step&nbsp;1 into a textual form suitable for prompting.\n",
    "2. Construct a prompt that:\n",
    "   * Instructs the model to generate BI questions similar in style to the three examples given by the user.\n",
    "   * Specifies that the `comment` column should be ignored and that extra weight should be placed on `updated_comment`.\n",
    "   * Embeds the full column list including their `updated_comment` field.\n",
    "3. Invoke the Databricks model-serving endpoint `databricks-meta-llama-3-1-405b-instruct` via `mlflow.deployments` to obtain exactly # questions.\n",
    "4. Display the generated questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e704b9c7-6a80-41e7-9e22-a62f564e8c23",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "define how many synthetic questions you want to generate"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "num_syn = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e5977e9-99c2-4d66-8256-5ed7ff9cf248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Only return the questions you synthesized, row by row, dont skip any row. Dont explain anything. Dont include anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5819f53-61e9-4954-b7f6-b23f63dfe6f5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "pipeline for synthesize using LLM"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from textwrap import dedent\n",
    "import os\n",
    "\n",
    "# Ensure combined_df exists\n",
    "df_pd = combined_df.select(\"col_name\", \"data_type\", \"updated_comment\", \"source_table\").toPandas()\n",
    "\n",
    "# Build metadata block for prompt\n",
    "def row_to_line(row):\n",
    "    return f\"{row['col_name']} ({row['data_type']}, source={row['source_table']}) - {row['updated_comment']}\"\n",
    "\n",
    "metadata_lines = \"\\n\".join(df_pd.apply(row_to_line, axis=1).tolist())\n",
    "\n",
    "example_questions = (\n",
    "    \"1. What is the highest eligible free rate for K-12 students in the schools in Alameda County?\\n\"\n",
    "    \"2. Please list the lowest three eligible free rates for students aged 5-17 in continuation schools.\\n\"\n",
    "    \"3. Please list the zip code of all the charter schools in Fresno County Office of Education.\"\n",
    ")\n",
    "\n",
    "\n",
    "system_prompt = dedent(f\"\"\"\n",
    "You are an expert data analyst tasked with crafting business-intelligence (BI) questions that can be answered from the provided school datasets.\n",
    "\n",
    "Requirements:\n",
    "* Use every column when relevant, but COMPLETELY IGNORE the field named `comment` because it is obsolete.\n",
    "* Place extra weight on the `updated_comment` field, which contains improved descriptions.\n",
    "* Produce exactly {num_syn} DISTINCT questions similar in style and complexity to the examples given.\n",
    "* Vary the aggregation (highest, lowest, average, count, percentage, etc.), filtering dimensions (county, school type, grade span, demographics, years, etc.), and output formats (list values, top-N, aggregated value).\n",
    "* Do NOT repeat the sample questions verbatim.\n",
    "* Number the questions 1-{num_syn}.\n",
    "* Do NOT generate any programming code.\n",
    "* Return the final result in a json dictionary format. Only return the dictionary as your output.\n",
    "\n",
    "\n",
    "Available columns with updated descriptions:\\n{metadata_lines}\n",
    "\n",
    "Sample question style:\\n{example_questions}\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Initialize deployments client\n",
    "try:\n",
    "    from mlflow import deployments as ml_deployments\n",
    "except ImportError:\n",
    "    import mlflow.deployments as ml_deployments\n",
    "\n",
    "client = ml_deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "chosen_endpoint = os.environ.get(\"LLM_ENDPOINT\", \"databricks-llama-4-maverick\")\n",
    "print(f\"Using endpoint: {chosen_endpoint}\")\n",
    "\n",
    "response = client.predict(\n",
    "    endpoint=chosen_endpoint,\n",
    "    inputs={\"messages\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Please generate the {num_syn} BI questions now.\"}\n",
    "    ]}\n",
    ")\n",
    "\n",
    "try:\n",
    "    questions_text = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "except Exception:\n",
    "    questions_text = str(response)\n",
    "\n",
    "print(\"Generated Questions:\\n\")\n",
    "print(questions_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb87f1b7-003a-4816-98ad-1277259814dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: if generate more than 200, say I tried 500, output above will skip many questions, it tends to use '...' in the answer. We have to use a loop to concatenate, or we can write an agent calling the above function tool to generate the answer set X times and concatenate with dedupe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d0de84-3db3-433d-b1ce-e4d1970644bf",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757665454638}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "dict to df"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "questions_text = questions_text.replace(\"```json\",\"\").replace(\"```\", \"\")\n",
    "\n",
    "questions_dict = json.loads(questions_text)\n",
    "questions_df = spark.createDataFrame([(k, v) for k, v in questions_dict.items()], [\"number\", \"query\"])\n",
    "display(questions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4d8cc98-dbbe-4766-99ad-0fef924a8557",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "if string output, formatting output to be a DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import Row\n",
    "\n",
    "# # Convert questions_text (numbered list) to list of questions\n",
    "# questions = [q.strip().split('. ', 1)[1] if '. ' in q else q.strip() for q in questions_text.split('\\n') if q.strip() and q[0].isdigit()]\n",
    "\n",
    "# # Create Spark DataFrame\n",
    "# questions_df = spark.createDataFrame([Row(query=q) for q in questions]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46fef2fc-786d-4ed4-915a-ed1aed4d5e1a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1757665538589}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "questions_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b061d5-a550-49eb-ab81-0fc3d33bb132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "questions_df.write.mode(\"append\") \\\n",
    "            .option(\"delta.enableChangeDataFeed\", \"false\") \\\n",
    "            .saveAsTable(\"yyang.hackathon_2025q3_project_geniepromptautocompletion_genieeng_feindustryhls.aisynthesized_questions_table\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c56e4a0-5358-42ae-927e-8ceb41659d72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3 - Update the existing delta table storing questions \n",
    "\n",
    "The managed VS index will sync on this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0fee8e-93ee-4411-b8f8-4b7ce9df5620",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "UPSERT Merge Operation"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "target_table = \"yyang.hackathon_2025q3_project_geniepromptautocompletion_genieeng_feindustryhls.questions_table\"\n",
    "\n",
    "# Get current max id from the target table\n",
    "max_id = spark.table(target_table).agg(F.max(\"id\")).collect()[0][0]\n",
    "if max_id is None:\n",
    "    max_id = 0\n",
    "\n",
    "# Add incremented id to questions_df\n",
    "questions_with_id = questions_df.withColumn(\"id\", F.monotonically_increasing_id() + max_id + 1)\n",
    "\n",
    "# Select only required columns for upsert\n",
    "upsert_df = questions_with_id.select(\"id\", \"query\")\n",
    "\n",
    "# Upsert into Delta table\n",
    "delta_table = DeltaTable.forName(spark, target_table)\n",
    "delta_table.alias(\"target\").merge(\n",
    "    upsert_df.alias(\"source\"),\n",
    "    \"target.query = source.query\"\n",
    ").whenNotMatchedInsert(\n",
    "  values={\"id\": \"source.id\", \"query\": \"source.query\"}\n",
    ").execute()\n",
    "\n",
    "# whenMatchedUpdate is skipped for above DeltaTable operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf21e81c-f694-4423-97f1-042d793414c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "check if n liines updated"
    }
   },
   "outputs": [],
   "source": [
    "spark.table(target_table).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b2777d-1bf1-4cb0-96f1-d8007f754ea3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4 - Sync the VS Index manually\n",
    "\n",
    "Depending on your setting of the VS Index update schedule, e.g., if 'Continous', you can skip below. But if you have \"Triggered\", please manually sync the VS index by running below cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35504827-2653-4bc0-a047-2c90e85fb244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-vectorsearch\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "262488a5-29d9-4141-8330-21d1f0a09da4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5605b349-220e-47fb-88e2-5c55d7e3d11f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"yyang.hackathon_2025q3_project_geniepromptautocompletion_genieeng_feindustryhls.questions_table\"\n",
    "\n",
    "vs_index_fullname = table_name.replace(\"questions_table\", \"questions_table_vs\")\n",
    "\n",
    "vs_endpoint_name = \"vs_endpoint_2025q3_hackathon_project_geniepromptautocompletion_genieeng_feindustryhls\".lower()[:49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aad4832-2d79-401a-aa00-ac1a4937417e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "index = vsc.get_index(endpoint_name=vs_endpoint_name, index_name=vs_index_fullname)\n",
    "\n",
    "index.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add95727-abad-46c1-ab28-14242fc3f8a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "index.sync()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6424631193817576,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02_Synthesize_Questions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
